{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Simple Neural Networks\n",
    "\n",
    "## 概要\n",
    "- Jupyter notebook上で一望できるくらい小さなニューラルネット(すぐ肥大化しそうな予感)\n",
    "- synthetic gradients試してみようと思って書きはじめた。勉強にもなるし\n",
    "- ReLU, Linear, Conv2D, SoftmaxCrossEntropyをひとまず実装(バグあるかも)\n",
    "\n",
    "## 注意\n",
    "- Conv2Dは縁の処理をサボっているのでちゃんとしたフレームワークとは挙動が違うかも\n",
    "- あんまり速くない\n",
    "- Python2\n",
    "\n",
    "## TODO\n",
    "- Pooling, BatchNormalizationの実装\n",
    "- weightのinitializeをもうちょっと丁寧に\n",
    "- gpu対応(私のPCにはNvidia GPUがないのでOpenCLを使う？)\n",
    "- RNNの記述法を考える\n",
    "- optimizerにmomentumを導入"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import math"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "class ReLU(object):\n",
    "\n",
    "    def __init__(self):\n",
    "        self.layer_type = 'activation'\n",
    "        \n",
    "    def forward(self, x):\n",
    "        self.x = x\n",
    "        return np.maximum(x, 0, dtype=x.dtype)\n",
    "    \n",
    "    def backward(self, gy):\n",
    "        return gy * (self.x > 0)\n",
    "\n",
    "class Linear(object):\n",
    "    \n",
    "    def __init__(self, inputs, outputs):\n",
    "        self.layer_type = 'linear'\n",
    "        self.W = np.random.uniform(-1/math.sqrt(inputs), 1/math.sqrt(inputs), (outputs, inputs)).astype('f')\n",
    "        self.b = np.zeros((outputs), dtype=np.float32)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        self.x = x\n",
    "        x = x.reshape(x.shape[0], -1)\n",
    "        y = x.dot(self.W.T) + self.b\n",
    "        return y\n",
    "    \n",
    "    def backward(self, gy):\n",
    "        x = self.x.reshape(self.x.shape[0], -1)\n",
    "        gx = gy.dot(self.W).reshape(self.x.shape)\n",
    "        self.gW = gy.T.dot(x)\n",
    "        self.gb = gy.sum(0)\n",
    "        return gx.reshape(self.x.shape)\n",
    "    \n",
    "class Convolution2D(object):\n",
    "    \n",
    "    def __init__(self, in_ch, out_ch, k, stride=1, pad=1):\n",
    "        self.layer_type = 'convolution'\n",
    "        self.ksize = k\n",
    "        self.stride = stride\n",
    "        self.pad = pad\n",
    "        self.W = np.random.uniform(-1/math.sqrt(k*k*in_ch), 1/math.sqrt(k*k*in_ch), (out_ch, in_ch, k, k)).astype('f')\n",
    "        self.b = np.zeros((out_ch), dtype=np.float32)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        self.x = x\n",
    "        b, ch, h, w = x.shape\n",
    "        p = self.pad\n",
    "        k = self.ksize\n",
    "        s = self.stride\n",
    "        \n",
    "        #padding input image\n",
    "        _x = np.zeros((b, ch, (h + p*2), (w + p*2)), dtype=np.float32)\n",
    "        _x[:, :, p:-p, p:-p] = x\n",
    "        \n",
    "        #im2col\n",
    "        self.col = np.zeros((b, ch, k, k, ((h + p*2 - k)//s + 1), ((w + p*2 - k)//s + 1)), dtype=np.float32)\n",
    "        for i in range(0, h + p*2 - k + 1, s):\n",
    "            for j in range(0, w + p*2 - k + 1, s):\n",
    "                self.col[:, :, :, :, i/s, j/s] += _x[:, :, i:i+k, j:j+k]\n",
    "        \n",
    "        #convolution\n",
    "        y = np.tensordot(self.col, self.W, ((1, 2, 3), (1, 2, 3))).astype(x.dtype, copy=False)\n",
    "        y += self.b\n",
    "        return np.rollaxis(y, 3, 1)\n",
    "    \n",
    "    def backward(self, gy):\n",
    "        self.gW = np.tensordot(gy, self.col, ((0, 2, 3), (0, 4, 5))).astype(self.W.dtype, copy=False)\n",
    "        self.gb = gy.sum(axis=(0, 2, 3))\n",
    "        gcol = np.tensordot(self.W, gy, (0, 1)).astype(self.x.dtype, copy=False)\n",
    "        gcol = np.rollaxis(gcol, 3)\n",
    "        \n",
    "        #col2im\n",
    "        b, ch, h, w = self.x.shape\n",
    "        p = self.pad\n",
    "        k = self.ksize\n",
    "        s = self.stride\n",
    "        gx = np.zeros((b, ch, (h + p*2), (w + p*2)), dtype=np.float32)\n",
    "        for i in range(0, h + p*2 - k + 1, s):\n",
    "            for j in range(0, w + p*2 - k + 1, s):\n",
    "                 gx[:, :, i:i+k, j:j+k] += gcol[:, :, :, :, i/s, j/s]\n",
    "        return gx[:, :, p:-p, p:-p]\n",
    "    \n",
    "def softmax(x):\n",
    "    x -= x.max(axis=1, keepdims=True)\n",
    "    exp_x = np.exp(x)\n",
    "    return exp_x/np.sum(exp_x, axis=1).reshape(-1, 1)\n",
    "\n",
    "def softmax_cross_entropy(x, t):\n",
    "    log_y = np.log(softmax(x))\n",
    "    log_p = log_y[range(len(t)), t.ravel()] #Labelに対応する値が1になる→log(y)=0．不正解Labelに対して期待される確率は0であるからそれらは無視できる．\n",
    "    loss = - log_p.sum() / len(t)\n",
    "    \n",
    "    gx = np.exp(log_y)\n",
    "    gx[range(len(t)), t.ravel()] -= 1\n",
    "    gx *= loss\n",
    "    \n",
    "    return loss, gx\n",
    "\n",
    "def accuracy(x, t):\n",
    "    t_or_f = (np.argmax(x, axis=1)==t).astype('f')\n",
    "    return np.sum(t_or_f)/len(t_or_f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def forward(nnet, x):\n",
    "    for layer in nnet:\n",
    "        x = layer.forward(x)\n",
    "    return x\n",
    "\n",
    "def backward(nnet, gy):\n",
    "    for layer in nnet[::-1]:\n",
    "        gy = layer.backward(gy)\n",
    "        \n",
    "def update(nnet):\n",
    "    lr = 0.001\n",
    "    for layer in nnet:\n",
    "        if layer.layer_type is not 'activation':\n",
    "            layer.W -= layer.gW * lr\n",
    "            layer.b -= layer.gb * lr"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training Test\n",
    "- mnistで試してみる"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import pylab as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# chainerのutilityをお借りしてmnistを読み込む\n",
    "import chainer\n",
    "train, test = chainer.datasets.get_mnist()\n",
    "\n",
    "X = np.zeros((60000, 1, 28, 28), dtype=np.float32)\n",
    "Y = np.zeros(60000, dtype=np.int32)\n",
    "X_test = np.zeros((10000, 1, 28, 28), dtype=np.float32)\n",
    "Y_test = np.zeros(10000, dtype=np.int32)\n",
    "for i in range(60000):\n",
    "    X[i] += train[i][0].reshape(1, 28, 28)\n",
    "    Y[i] = train[i][1]\n",
    "for i in range(10000):\n",
    "    X_test[i] += test[i][0].reshape(1, 28, 28)\n",
    "    Y_test[i] = test[i][1]\n",
    "    \n",
    "#[-1, 1]に正規化\n",
    "X -= 0.5; X_test -= 0.5\n",
    "X *= 2; X_test *= 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#モデルの準備\n",
    "\n",
    "nnet = [Convolution2D(1, 32, 4, stride=2, pad=1),\n",
    "        ReLU(),\n",
    "        Convolution2D(32, 64, 3, stride=1, pad=1),\n",
    "        ReLU(),\n",
    "        Convolution2D(64, 128, 4, stride=2, pad=1),\n",
    "        ReLU(),\n",
    "        Convolution2D(128, 256, 3, stride=2, pad=1),\n",
    "        ReLU(),\n",
    "        Linear(256*4*4, 512),\n",
    "        ReLU(),\n",
    "        Linear(512, 10)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 0\n",
      "type: train, sample: 10100, loss: 1.77315624008, accuracy: 0.4031\n",
      "type: train, sample: 20100, loss: 0.390982931328, accuracy: 0.8817\n",
      "type: train, sample: 30100, loss: 0.26652086134, accuracy: 0.9231\n",
      "type: train, sample: 40100, loss: 0.229616665363, accuracy: 0.9303\n",
      "type: train, sample: 50100, loss: 0.198822689676, accuracy: 0.9405\n",
      "type: val, sample: 60000, loss: 0.156490174598, accuracy: 0.9508\n",
      "epoch: 1\n",
      "type: train, sample: 70100, loss: 0.160351283455, accuracy: 0.9598\n",
      "type: train, sample: 80100, loss: 0.142623968077, accuracy: 0.9567\n",
      "type: train, sample: 90100, loss: 0.149283674908, accuracy: 0.9553\n",
      "type: train, sample: 100100, loss: 0.137186365223, accuracy: 0.9595\n",
      "type: train, sample: 110100, loss: 0.126101793265, accuracy: 0.9608\n",
      "type: val, sample: 120000, loss: 0.116489330018, accuracy: 0.9633\n",
      "epoch: 2\n",
      "type: train, sample: 130100, loss: 0.113217493129, accuracy: 0.9744\n",
      "type: train, sample: 140100, loss: 0.122570774388, accuracy: 0.9629\n",
      "type: train, sample: 150100, loss: 0.112114821458, accuracy: 0.9671\n",
      "type: train, sample: 160100, loss: 0.101626035404, accuracy: 0.9697\n"
     ]
    }
   ],
   "source": [
    "epoch = 100\n",
    "N = len(X)\n",
    "N_test = len(X_test)\n",
    "batchsize = 100\n",
    "step = 100\n",
    "n_imgs_trained = 0\n",
    "\n",
    "for e in range(epoch):\n",
    "    print 'epoch:', e\n",
    "    sum_loss = 0.\n",
    "    sum_acc = 0.\n",
    "    perm = np.random.permutation(N)\n",
    "    \n",
    "    #train\n",
    "    for i in range(0, N, batchsize):\n",
    "        y = forward(nnet, X[perm[i:i+batchsize]])\n",
    "        loss, gy = softmax_cross_entropy(y, Y[perm[i:i+batchsize]])\n",
    "        acc = accuracy(y, Y[perm[i:i+batchsize]])\n",
    "        backward(nnet, gy)\n",
    "        update(nnet)\n",
    "        \n",
    "        n_imgs_trained += batchsize\n",
    "        sum_loss += loss\n",
    "        sum_acc += acc\n",
    "        \n",
    "        if i%(batchsize*step)==0 and i!=0:\n",
    "            print 'type: train, sample: {}, loss: {}, accuracy: {}'.format(n_imgs_trained, sum_loss/step, sum_acc/step)\n",
    "            sum_loss = 0.\n",
    "            sum_acc = 0.\n",
    "    #val\n",
    "    sum_loss = 0.\n",
    "    sum_acc = 0.\n",
    "    for i in range(0, N_test, batchsize):\n",
    "        y = forward(nnet, X_test[i:i+batchsize])\n",
    "        loss, gy = softmax_cross_entropy(y, Y_test[i:i+batchsize])\n",
    "        acc = accuracy(y, Y_test[i:i+batchsize])\n",
    "        sum_loss += loss\n",
    "        sum_acc += acc\n",
    "    print 'type: val, sample: {}, loss: {}, accuracy: {}'.format(n_imgs_trained, sum_loss/(N_test/batchsize), sum_acc/(N_test/batchsize))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
